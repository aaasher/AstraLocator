{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import h3\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_orgs_yamaps(super_cat: str,\n",
    "                    sub_cat: str,\n",
    "                    skip: int,\n",
    "                    bbox: str='37.521587,55.707375~37.712390,55.796668',\n",
    "                   ) -> Tuple[DataFrame, bool]:\n",
    "    \n",
    "    # make api request\n",
    "    ymaps_api = '5b654382-731c-4c23-80eb-f3c24084460d'\n",
    "    \n",
    "    request_url = (\n",
    "        f'https://search-maps.yandex.ru/v1/'\n",
    "        f'?apikey={ymaps_api}'\n",
    "        f'&text={sub_cat}'\n",
    "        f'&lang=ru_RU'\n",
    "        f'&type=biz'\n",
    "        f'&bbox={bbox}'\n",
    "        f'&rspn=1'\n",
    "        f'&skip={skip}'\n",
    "        f'&results=500'\n",
    "    )\n",
    "    \n",
    "    resp = requests.get(request_url)\n",
    "    print(sub_cat, skip, 'resp_code', resp.status_code)\n",
    "\n",
    "    # extract valid data from json\n",
    "    source = []\n",
    "    data = resp.json()\n",
    "    features = data.get('features')\n",
    "    if features:\n",
    "        print(sub_cat, skip, 'count', len(features))\n",
    "        \n",
    "        for feature in features:\n",
    "            geo_point = feature.get('geometry', {}).get('coordinates', [])\n",
    "            ya_id = feature.get('properties', {}).get('CompanyMetaData', {}).get('id', None)\n",
    "            name = feature.get('properties', {}).get('CompanyMetaData', {}).get('name', None)\n",
    "            address = feature.get('properties', {}).get('CompanyMetaData', {}).get('address', None)\n",
    "            url = feature.get('properties', {}).get('CompanyMetaData', {}).get('url', None)\n",
    "\n",
    "            phone_arr = [\n",
    "                ph.get('formatted')\n",
    "                for ph in feature.get('properties', {}).get('CompanyMetaData', {}).get('Phones', [{}])\n",
    "                if ph\n",
    "            ]\n",
    "\n",
    "            class_arr = [\n",
    "                cl.get('class')\n",
    "                for cl in feature.get('properties', {}).get('CompanyMetaData', {}).get('Categories', [{}])\n",
    "                if cl\n",
    "            ]\n",
    "\n",
    "            categories_arr = [\n",
    "                ct.get('name')\n",
    "                for ct in feature.get('properties', {}).get('CompanyMetaData', {}).get('Categories', [{}])\n",
    "                if ct\n",
    "            ]\n",
    "\n",
    "            hours_text = feature.get('properties', {}).get('CompanyMetaData', {}).get('Hours', {}).get('text', None)\n",
    "\n",
    "            temp = (\n",
    "                super_cat,\n",
    "                sub_cat,\n",
    "                geo_point,\n",
    "                ya_id,\n",
    "                name,\n",
    "                address,\n",
    "                url,\n",
    "                phone_arr,\n",
    "                class_arr,\n",
    "                categories_arr,\n",
    "                hours_text\n",
    "            )\n",
    "            source.append(temp)\n",
    "\n",
    "    # convert to dataframe\n",
    "    columns = [\n",
    "        'super_cat',\n",
    "        'sub_cat',\n",
    "        'geo_point',\n",
    "        'ya_id',\n",
    "        'name',\n",
    "        'address',\n",
    "        'url',\n",
    "        'phone_arr',\n",
    "        'class_arr',\n",
    "        'categories_arr',\n",
    "        'hours_text'\n",
    "    ]\n",
    "    df = pd.DataFrame(source,columns=columns)\n",
    "    \n",
    "    # check if there is more data left to query\n",
    "    found = data.get('properties',{}).get('ResponseMetaData',{}).get('SearchResponse',{}).get('found',0)\n",
    "    results = data.get('properties',{}).get('ResponseMetaData',{}).get('SearchRequest',{}).get('results',0)\n",
    "    skip = data.get('properties',{}).get('ResponseMetaData',{}).get('SearchRequest',{}).get('skip',0)\n",
    "    \n",
    "    if skip + results < found:\n",
    "        is_more = True\n",
    "    else:\n",
    "        is_more = False\n",
    "        \n",
    "    return df, is_more\n",
    "\n",
    "\n",
    "\n",
    "def get_orgs_yamaps_all(super_cat: str,\n",
    "                        sub_cat: str,\n",
    "                        bbox: str='37.521587,55.707375~37.712390,55.796668',\n",
    "                       )-> DataFrame:\n",
    "    \n",
    "    skip = 0\n",
    "    is_more = True\n",
    "    dfs = []\n",
    "    \n",
    "    while is_more:\n",
    "        df, is_more = get_orgs_yamaps(super_cat=super_cat,\n",
    "                                      sub_cat=sub_cat,\n",
    "                                      skip=skip,\n",
    "                                      bbox=bbox)\n",
    "        dfs.append(df)\n",
    "        skip += 500\n",
    "        print('skip += 500')\n",
    "        \n",
    "    return pd.concat(dfs)\n",
    "        \n",
    "\n",
    "def get_orgs_categories(json_path: str='bcats_raw.json'):\n",
    "    with open(json_path) as data_file:\n",
    "        data_loaded = json.load(data_file)\n",
    "    return [(cat, sub) for cat,sub in data_loaded.items()]\n",
    "\n",
    "\n",
    "    \n",
    "def get_save_all_orgs(path: str='temp/'):\n",
    "    \n",
    "    cats = get_orgs_categories()\n",
    "    \n",
    "    names = []\n",
    "    for super_cat in cats:\n",
    "        print('getting super...', super_cat[0])\n",
    "        for sub_cat in super_cat[1]:\n",
    "            \n",
    "            print(' getting sub...', sub_cat)\n",
    "            df = get_orgs_yamaps_all(super_cat=super_cat[0], sub_cat=sub_cat)\n",
    "\n",
    "            filename = f'{path}{super_cat[0]}_{sub_cat}.csv'\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(' success...', filename)\n",
    "            names.append(filename)\n",
    "            \n",
    "    return names\n",
    "\n",
    "\n",
    "def read_saved(names):\n",
    "    return pd.concat([pd.read_csv(name) for name in names])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_coord_grid(left_bottom: list=[37.521587, 55.707375],\n",
    "                    right_top: list =[37.712390, 55.796668],\n",
    "                    x_count: int=100,\n",
    "                    y_count: int=100):\n",
    "    \"\"\"\n",
    "    moscow ttk: '37.521587,55.707375~37.712390,55.796668'\n",
    "    \n",
    "    \"\"\"\n",
    "    # make linearly spaced coordinate grid\n",
    "    xmin, xmax, ymin, ymax = left_bottom[0], right_top[0], left_bottom[1], right_top[1]\n",
    "    xx, yy = np.meshgrid(np.linspace(xmin,xmax,x_count), np.linspace(ymin,ymax,y_count))\n",
    "    # make coordinate grid hexagonal by shifting each two lines\n",
    "    # works when x_count = y_count = 100\n",
    "    xx[::2, :] += 0.001\n",
    "    \n",
    "    grid = np.hstack([yy.reshape(x_count*y_count,1),\n",
    "                      xx.reshape(x_count*y_count,1)])\n",
    "    \n",
    "    # make kdtree for effective grid search\n",
    "    tree = BallTree(np.deg2rad(grid), metric='haversine')\n",
    "    \n",
    "    return grid, tree\n",
    "\n",
    "\n",
    "def load_coord_grid(path=None):\n",
    "    if path is None:\n",
    "        path = os.getcwd()+'/'+'grid.csv'\n",
    "    grid = pd.read_csv(path).iloc[:,1:].to_numpy()\n",
    "    tree = BallTree(np.deg2rad(grid), metric='haversine')\n",
    "    return grid, tree\n",
    "\n",
    "grid, tree = load_coord_grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_rent_yamaps(lat: float,\n",
    "                    lon: float):\n",
    "    api_url = (\n",
    "        f'https://realty.yandex.ru/api/1.0/heatmap/price-rent/point/value'\n",
    "        f'?latitude={lat}'\n",
    "        f'&longitude={lon}'\n",
    "    )\n",
    "    data = requests.get(api_url).json()\n",
    "    data = data.get('response', {})\n",
    "    return (\n",
    "        data.get('valueDescription'),\n",
    "        data.get('valueFrom'),\n",
    "        data.get('valueTo'),\n",
    "        data.get('level'),\n",
    "        data.get('maxLevel'),\n",
    "        data.get('realValue')\n",
    "    )\n",
    "\n",
    "\n",
    "def get_rent_yamaps_all(grid: np.ndarray):\n",
    "    \n",
    "    source = []\n",
    "    \n",
    "    for idx, p in enumerate(grid):\n",
    "        rent = get_rent_yamaps(lat=p[0], lon=p[1])\n",
    "        print(tuple(p) + rent)\n",
    "        source.append(tuple(p) + rent)\n",
    "        # sleep(0.01)\n",
    "    \n",
    "    columns = [\n",
    "        'point_index',\n",
    "        'lat',\n",
    "        'lon',\n",
    "        'valueDescription',\n",
    "        'valueFrom',\n",
    "        'valueTo',\n",
    "        'level',\n",
    "        'maxLevel',\n",
    "        'realValue'\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(source, columns=columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traffic(files: list):\n",
    "    for idx, file in enumerate(files):\n",
    "        # read csv ~ 50 mln rows, required ~ 2.5 GB memory\n",
    "        df_traffic_0 = pd.read_csv(file)\n",
    "        print(idx, 'read csv')\n",
    "        # ts to datetime\n",
    "        df_traffic_0['ts'] = pd.to_datetime(df_traffic_0['ts'])\n",
    "        print(idx, 'ts to datetime')\n",
    "        # gps to lat lon floats, required ~ 20 GB memory (physical + swap)\n",
    "        coords = df_traffic_0['gps'].str.split(',', expand=True)\n",
    "        print(idx, 'gps to lat lon floats - split')\n",
    "        coords.columns = ['lat', 'lon']\n",
    "        coords['lat'] = coords['lat'].str.replace('(','').astype(float)\n",
    "        print(idx, 'gps to lat lon floats - replace 1')\n",
    "        coords['lon'] = coords['lon'].str.replace(')','').astype(float)\n",
    "        print(idx, 'gps to lat lon floats - replace 2')\n",
    "        df_traffic_0 = pd.concat([df_traffic_0[['ts', 'ap_mac', 'device_id', 'user_id']], coords], axis=1)\n",
    "        print(idx, 'gps to lat lon floats - concat')\n",
    "        del coords\n",
    "        \n",
    "        # get unique coordinates of wi-fi points \n",
    "        unique_coords = df_traffic_0[['ap_mac', 'lat', 'lon']].drop_duplicates(subset=['lat', 'lon'], keep='first')\n",
    "        \n",
    "        # create hour column\n",
    "        df_traffic_0['ts_h'] =  df_traffic_0['ts'].dt.round('H')\n",
    "        \n",
    "        # match wi-fi point coordinates with our grid\n",
    "        coords_matrix = unique_coords[['lat', 'lon']].to_numpy()\n",
    "        dist, indexes = tree.query(np.deg2rad(coords_matrix))\n",
    "        points = np.append(indexes, grid[indexes.ravel()], axis=1)\n",
    "        points = np.hstack([coords_matrix, points, dist])\n",
    "        points = pd.DataFrame(points, columns=['lat', 'lon', 'point_index', 'point_lat', 'point_lon', 'dist'])\n",
    "        points = pd.merge(unique_coords, points, on=['lat', 'lon'], how='left')\n",
    "        points = points[points['dist']<=points['dist'].quantile(.90)]\n",
    "        points_join = points[['ap_mac', 'point_index']]\n",
    "        # for faster join - create index and sort it\n",
    "        df_traffic_0.set_index('ap_mac', inplace=True)\n",
    "        df_traffic_0.sort_index(inplace=True)\n",
    "        # for faster join - create index and sort it\n",
    "        points_join.set_index('ap_mac', inplace=True)\n",
    "        points_join.sort_index(inplace=True)\n",
    "        # join\n",
    "        df_traffic_0 = df_traffic_0.join(points_join, how='left')\n",
    "        \n",
    "        # group by grid point, hour and count devices\n",
    "        df_traffic_agg = df_traffic_0.groupby(\n",
    "            ['point_index', 'ts_h']\n",
    "        ).agg(\n",
    "            count_device=('device_id', 'count'),\n",
    "            unique_device=('device_id', 'nunique')\n",
    "        ).reset_index()\n",
    "        \n",
    "        # save results ~ 0.5 mln rows, 20 Mb\n",
    "        df_traffic_agg.to_csv(f'df_traffic_agg_{idx}.csv', index=False)\n",
    "        \n",
    "        \n",
    "def preprocess_traffic(files):\n",
    "    \n",
    "    df_agg_all = read_saved(files)\n",
    "     \n",
    "    # create day column\n",
    "    df_agg_all['ts_h'] = pd.to_datetime(df_agg_all['ts_h'])\n",
    "    df_agg_all['ts_d'] = df_agg_all['ts_h'].dt.round('D')\n",
    "    df_agg_all['is_night'] = df_agg_all['ts_h'].apply(lambda x: 1 if ((x.hour>=22) or (x.hour<=7)) else 0)\n",
    "\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    dta = [('all', df_agg_all),\n",
    "           ('night', df_agg_all[df_agg_all['is_night']==1]),\n",
    "           ('day', df_agg_all[df_agg_all['is_night']==0])]\n",
    "\n",
    "    for group, df_agg in dta:\n",
    "\n",
    "        # groupby point and date, sum unique devices\n",
    "        df_agg = df_agg.groupby(\n",
    "            ['point_index', 'ts_d']\n",
    "        ).agg( \n",
    "            unique_device=('unique_device', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        # group only by point, and count median of unique devices per point from 03-05 month\n",
    "        df_agg = df_agg.groupby(\n",
    "            ['point_index']\n",
    "        ).agg(\n",
    "            unique_device=('unique_device', 'median')\n",
    "        ).reset_index()\n",
    "\n",
    "        # add lat lon columns from grid\n",
    "        df_gr = pd.DataFrame(grid).reset_index()\n",
    "        df_gr.columns=['point_index', 'point_lat', 'point_lon']\n",
    "        df_fin = pd.merge(df_gr, df_agg, on=['point_index'], how='left').fillna(0)\n",
    "\n",
    "        # blur points = make every point a sum from 18 surrounding points\n",
    "        blur = 18\n",
    "        if blur > 0:\n",
    "            indexes = tree.query(np.deg2rad(df_fin[['point_lat', 'point_lon']].to_numpy()), k=blur)[1]\n",
    "            df_fin.index = df_fin['point_index'].astype(int)\n",
    "            rank_matrix = np.concatenate([df_fin[['unique_device']].reindex(indexes[:,idx]).to_numpy()\n",
    "                                          for idx in range(0,blur)],\n",
    "                                         axis=1)\n",
    "            df_fin['unique_device'] = np.sum(rank_matrix, axis=1)\n",
    "\n",
    "        # save\n",
    "        df_fin = df_fin.reset_index(drop=True)\n",
    "        df_fin.columns = ['point_index', 'point_lat', 'point_lon', 'metric_traffic']\n",
    "        df_fin.insert(0, 'group', group)\n",
    "        dfs.append(df_fin)\n",
    "\n",
    "    dta = pd.concat(dfs)\n",
    "    dta.to_csv('metric_traffic.csv', index=False)\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_competition():\n",
    "    path = os.getcwd()+'/temp/'\n",
    "    # PARSING, takes time\n",
    "    get_save_all_orgs(path)\n",
    "    \n",
    "    names = [path+name \n",
    "             for name in os.listdir(path) if '.csv' in name]\n",
    "\n",
    "    df = read_saved(names)\n",
    "    df['lat'] = df['geo_point'].apply(lambda x: json.loads(x)[1])\n",
    "    df['long'] = df['geo_point'].apply(lambda x: json.loads(x)[0])\n",
    "    df.to_csv('competition_data.csv', index=False)\n",
    "    \n",
    "    return os.getcwd()+'/'+'competition_data.csv'\n",
    "    \n",
    "\n",
    "def create_metric_rent():\n",
    "    # PARSING, takes time\n",
    "    df_rent = get_rent_yamaps_all(grid)\n",
    "    \n",
    "    df_rt = df_rent[~df_rent['realValue'].isna()][['lat', 'lon', 'realValue']]\n",
    "    df_rt.rename(columns={'realValue': 'metric_rent'}, inplace=True)\n",
    "    df_rt.to_csv('metric_rent.csv', index=False)\n",
    "    return os.getcwd()+'/'+'metric_rent.csv'\n",
    "\n",
    "\n",
    "def haversine_exc(point_1, point_2=(55.7525, 37.6231)):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [point_1[1], point_1[0], point_2[1], point_2[0]])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    return 6367 * 2 * np.arcsin(np.sqrt(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_with_grid_points(df, columns=['lat', 'long']):\n",
    "    indexes = tree.query(np.deg2rad(df[columns].to_numpy()))[1]\n",
    "    points = np.append(indexes, grid[indexes.ravel()], axis=1)\n",
    "    return pd.concat([df,\n",
    "                      pd.DataFrame(points, columns=['point_index', 'point_lat', 'point_lon'])],\n",
    "                     axis=1)\n",
    "\n",
    "def preprocess_data():\n",
    "    # read\n",
    "    competition_data = pd.read_csv('competition_data.csv')\n",
    "    metric_rent = pd.read_csv('metric_rent.csv')\n",
    "    metric_traffic = pd.read_csv('metric_traffic.csv')\n",
    "    \n",
    "    # all dfs would be mapped to points\n",
    "    metric_rent = metric_rent.reset_index().rename(columns={'index':'point_index',\n",
    "                                             'lat': 'point_lat',\n",
    "                                             'lon': 'point_lon'})\n",
    "\n",
    "    metric_traffic = metric_traffic.reset_index().rename(columns={'index':'point_index',\n",
    "                                             'lat': 'point_lat',\n",
    "                                             'lon': 'point_lon'})\n",
    "    \n",
    "    competition_data = match_with_grid_points(competition_data,\n",
    "                                              columns=['lat', 'long'])\n",
    "    \n",
    "    competition_data.to_csv('competition_data.csv', index=False)\n",
    "    metric_rent.to_csv('metric_rent.csv', index=False)\n",
    "    metric_traffic.to_csv('metric_traffic.csv', index=False)\n",
    "    metric_rent[['point_index', 'point_lat', 'point_lon']].to_csv('grid.csv', index=False)\n",
    "    \n",
    "    return [os.getcwd()+'/'+'competition_data.csv', os.getcwd()+'/'+'metric_rent.csv',\\\n",
    "            os.getcwd()+'/'+'metric_traffic.csv', os.getcwd()+'/'+'grid.csv']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def point_to_hex(lat, lon):\n",
    "    \n",
    "    coord_matrix = [\n",
    "        [cords[1], cords[0]]\n",
    "        for cords in h3.h3_to_geo_boundary(h3.geo_to_h3(lat, lon, 10))\n",
    "    ]\n",
    "\n",
    "    return coord_matrix + [coord_matrix[0]]\n",
    "\n",
    "\n",
    "def df_to_geojson(df, top=10000):\n",
    "    geojson = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': [\n",
    "            {'type': 'Feature',\n",
    "             'geometry': {\n",
    "                 'type': 'Polygon',\n",
    "                 'coordinates': [\n",
    "                     point_to_hex(row.point_lat, row.point_lon)\n",
    "                 ]},\n",
    "             'properties': {\n",
    "                 'point_index': row.point_index,\n",
    "                 'point_lat': row.point_lat,\n",
    "                 'point_lon': row.point_lon,\n",
    "                 'metric_rent': row.metric_rent,\n",
    "                 'metric_traffic': row.metric_traffic,\n",
    "                 'dist': row.dist,\n",
    "                 'count_comps': row.count_comps,\n",
    "                 'rank_rent': row.rank_rent,\n",
    "                 'rank_traffic': row.rank_traffic,\n",
    "                 'rank_comps': row.rank_comps,\n",
    "                 'total_rank': row.total_rank\n",
    "             },\n",
    "             'id': row.point_index\n",
    "            }\n",
    "            for idx, row in enumerate(df.itertuples()) if idx <= top\n",
    "        ]\n",
    "    }\n",
    "    return geojson\n",
    "\n",
    "def geojson_to_df(geojson):\n",
    "    return pd.DataFrame([feature['properties'] for feature in geojson['features']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "competition_data = pd.read_csv('competition_data.csv')\n",
    "competition_data['comps_array'] = np.array(competition_data.to_dict('records'))\n",
    "\n",
    "metric_rent = pd.read_csv('metric_rent.csv')\n",
    "metric_rent['dist'] = metric_rent.apply(lambda x: haversine_exc([x['point_lat'], x['point_lon']]), axis=1)\n",
    "\n",
    "metric_traffic = pd.read_csv('metric_traffic.csv')\n",
    "\n",
    "\n",
    "def find_optimal(sub_cat,\n",
    "                 imp_rent,\n",
    "                 imp_traffic,\n",
    "                 imp_comps,\n",
    "                 center_dist,\n",
    "                 top,\n",
    "                 blur,\n",
    "                 hours):\n",
    "    \n",
    "    # leave only certain hours traffic data\n",
    "    df_ranked = metric_traffic[metric_traffic['group']==hours].iloc[:, 1:]\n",
    "    \n",
    "    # merge with rent data\n",
    "    df_ranked = pd.merge(metric_rent, df_ranked[['point_index', 'metric_traffic']],\n",
    "                         on='point_index', how='left')\n",
    "\n",
    "    # leave only certain points, that are X km far from city center\n",
    "    df_ranked = df_ranked[df_ranked['dist'] < center_dist]\n",
    "    \n",
    "    # df_ranked = df_ranked[df_ranked['metric_traffic']>0]\n",
    "    \n",
    "    # leave only certain busnesses here\n",
    "    count_comps = competition_data[\n",
    "        competition_data['sub_cat'].isin(sub_cat)\n",
    "    ][\n",
    "        ['point_index', 'ya_id', 'comps_array']\n",
    "    ].groupby(\n",
    "        'point_index'\n",
    "    ).agg({\n",
    "        'ya_id': 'nunique',\n",
    "        'comps_array': list\n",
    "    }).reset_index().rename(columns={'ya_id': 'count_comps'})\n",
    "\n",
    "    df_ranked = pd.merge(df_ranked, count_comps, on='point_index', how='left').fillna(0)\n",
    "    \n",
    "    # apply blur to count comps\n",
    "    if blur > 0:\n",
    "        indexes = tree.query(np.deg2rad(df_ranked[['point_lat', 'point_lon']].to_numpy()), k=blur)[1]\n",
    "        \n",
    "        df_ranked.index = df_ranked['point_index'].astype(int)\n",
    "        \n",
    "        rank_matrix = np.concatenate([df_ranked[['count_comps']].reindex(indexes[:,idx]).to_numpy()\n",
    "                                      for idx in range(0,blur)],\n",
    "                                     axis=1)\n",
    "        \n",
    "        df_ranked['count_comps'] = np.sum(rank_matrix, axis=1)\n",
    "    \n",
    "\n",
    "    # compute weighted rating\n",
    "    tot = sum([imp_rent, imp_traffic, imp_comps])\n",
    "    w_r, w_t, w_c = imp_rent/tot, imp_traffic/tot, imp_comps/tot\n",
    "\n",
    "    df_ranked['rank_rent'] = (1/(df_ranked['metric_rent']+1)).rank(pct=True)\n",
    "    df_ranked['rank_traffic'] = df_ranked['metric_traffic'].rank(pct=True)\n",
    "    df_ranked['rank_comps'] = (1/(df_ranked['count_comps']+1)).rank(pct=True)\n",
    "        \n",
    "    \n",
    "    \n",
    "    df_ranked['total_rank'] = ( w_r * df_ranked['rank_rent'] \\\n",
    "                              + w_t * df_ranked['rank_traffic'] \\\n",
    "                              + w_c * df_ranked['rank_comps'] ).round(5)\n",
    "    \n",
    "\n",
    "    df_ranked = df_ranked.sort_values(by='total_rank', ascending=False)[:top]\n",
    "    \n",
    "    # convert to geojson\n",
    "    return df_to_geojson(df_ranked)\n",
    "    \n",
    "print('ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top_lat = 55.774497\n",
    "# bottom_lat = 55.728464\n",
    "\n",
    "# top_long = 37.660644\n",
    "# bottom_long = 37.582188\n",
    "\n",
    "competition_data.groupby('sub_cat')['ya_id'].count().sort_values(ascending=False).reset_index()['sub_cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joury_mapping = {\n",
    "    \"Фуд ритейл\": ['Магазин продуктов', 'Супермаркет', 'Магазин овощей и фруктов', 'Гипермаркет', 'Продуктовый гипермаркет'],\n",
    "    \"Парикмахерские, салоны красоты и т.п.\": ['Салон красоты', 'Парикмахерская', 'Барбершоп'],\n",
    "    \"Рестораны, кафе, бары и ночные клубы\": ['Ресторан', 'Кафе', 'Ночной клуб', 'Бар, паб'],\n",
    "    \"Бытовая химия, магазины косметики и т.п.\": ['Магазин хозтоваров и бытовой химии'],\n",
    "    \"Клиники, стоматологии, вет.клиники\": ['Медцентр', 'Поликлиника', 'Стоматологическая клиника', 'Ветеринарная клиника'],\n",
    "    \"Услуги (кофе на вынос, ремонт обуви и ювелирных изделий) и мелкая торговля (фото, пресса, мороженое).\": [\n",
    "        'Ремонт одежды',\n",
    "        'Ремонт обуви',\n",
    "        'Ремонт часов',\n",
    "        'Ремонт бытовой техники',\n",
    "        'Мороженое',\n",
    "        'Кофейня',\n",
    "        'Фотомагазин',\n",
    "        'Магазин смешанных товаров',\n",
    "        'Магазин чая и кофе'\n",
    "    ]\n",
    "}\n",
    "\n",
    "joury_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Фуд ритейл\n",
    "2. Парикмахерские, салоны красоты и т.п.\n",
    "3. Рестораны, кафе, бары и ночные клубы\n",
    "4. Бытовая химия, магазины косметики и т.п.\n",
    "5. Клиники, стоматологии, вет.клиники\n",
    "6. Услуги (кофе на вынос, ремонт обуви и ювелирных изделий) и мелкая торговля (фото, пресса, мороженое)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_box_token = 'pk.eyJ1IjoiYXNoZXJhc2hlciIsImEiOiJja3BncjV0d28yZHVvMnBsbGx5aGp4d3lrIn0.sqhH34AnR9FQCC2tysD3vg'\n",
    "px.set_mapbox_access_token(map_box_token)\n",
    "\n",
    "sub_cat = ['Магазин продуктов']#joury_mapping['Парикмахерские, салоны красоты и т.п.']\n",
    "imp_rent = 0\n",
    "imp_traffic= 0\n",
    "imp_comps = 10\n",
    "center_dist = 12.2\n",
    "top = 100000\n",
    "blur = 18\n",
    "hours='night'\n",
    "\n",
    "geojs = find_optimal(sub_cat,\n",
    "                     imp_rent,\n",
    "                     imp_traffic,\n",
    "                     imp_comps,\n",
    "                     center_dist,\n",
    "                     top,\n",
    "                     blur,\n",
    "                     hours)\n",
    "\n",
    "fig = px.choropleth_mapbox(geojson_to_df(geojs),\n",
    "                           geojson=geojs,\n",
    "                           color=\"total_rank\",\n",
    "                           locations=\"point_index\",\n",
    "                           opacity=0.8,\n",
    "                           color_continuous_scale=\"plotly3_r\",\n",
    "                           zoom=11.0,\n",
    "                           center={\n",
    "                               \"lat\": 55.7525,\n",
    "                               \"lon\": 37.6231\n",
    "                           },\n",
    "                           hover_data=[\n",
    "                               'point_index',\n",
    "                               'point_lat',\n",
    "                               'point_lon',\n",
    "                               'metric_rent',\n",
    "                               'metric_traffic',\n",
    "                               'count_comps',\n",
    "                               \n",
    "                               'total_rank'\n",
    "                           ]\n",
    "                          )\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(margin=dict(b=0, t=0, l=0, r=0))\n",
    "name = '_'.join(sub_cat) + f'_top={top}_blur={blur}_wrnt={imp_rent}_wtrf={imp_traffic}__wcmp={imp_comps}'\n",
    "print(name)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('geojs.json', 'w') as f:\n",
    "    json.dump(geojs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(f'{name}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_traffic_hours_ts(files):\n",
    "    df_agg = read_saved(files)\n",
    "\n",
    "    # create day column\n",
    "    df_agg['ts_h'] = pd.to_datetime(df_agg['ts_h'])\n",
    "    df_agg['hour'] = df_agg['ts_h'].dt.hour\n",
    "    # df_agg['weekday'] = df_agg['ts_h'].dt.weekday\n",
    "\n",
    "\n",
    "    df_agg = df_agg.groupby(['point_index', 'hour'])['unique_device'].median().reset_index()\n",
    "\n",
    "\n",
    "    # add lat lon columns from grid\n",
    "    df_gr = pd.DataFrame(grid).reset_index()\n",
    "    df_gr.columns=['point_index', 'point_lat', 'point_lon']\n",
    "    df_fin = pd.merge(df_gr, df_agg, on=['point_index'], how='left').fillna(0)\n",
    "\n",
    "    # pivot - unpivot to fill missing hour values\n",
    "    df_fin = pd.pivot_table(df_fin,\n",
    "                            values='unique_device',\n",
    "                            index='point_index',\n",
    "                            columns='hour',\n",
    "                            fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "    df_fin = pd.melt(df_fin,\n",
    "                     id_vars='point_index',\n",
    "                     value_name='unique_device')\n",
    "\n",
    "    # check: df_fin[df_fin['point_index']==2914].sort_values(by='hour')\n",
    "\n",
    "    # add lat lon\n",
    "    df_fin = pd.merge(df_fin, df_gr, on=['point_index'], how='left')\n",
    "\n",
    "    dfs = []\n",
    "    for hour in df_fin['hour'].unique():\n",
    "        print(hour)\n",
    "        # get only values for specific hour\n",
    "        dff = df_fin[df_fin['hour']==hour]\n",
    "        # blur points = make every point a sum from 6 surrounding points\n",
    "        blur = 18\n",
    "        if blur > 0:\n",
    "            indexes = tree.query(np.deg2rad(dff[['point_lat', 'point_lon']].to_numpy()),\n",
    "                                 k=blur)[1]\n",
    "            dff.index = dff['point_index'].astype(int)\n",
    "            rank_matrix = np.concatenate([dff[['unique_device']].reindex(indexes[:,idx]).to_numpy()\n",
    "                                          for idx in range(0,blur)],\n",
    "                                         axis=1)\n",
    "            dff['unique_device'] = np.sum(rank_matrix, axis=1)\n",
    "\n",
    "        # save\n",
    "        dff = dff.reset_index(drop=True)\n",
    "        dff = dff[['hour', 'point_index', 'point_lat', 'point_lon', 'unique_device']]\n",
    "        dff.rename(columns={'unique_device': 'metric_traffic'})\n",
    "        dfs.append(dff)\n",
    "\n",
    "    df_final = pd.concat(dfs)\n",
    "    df_final.to_csv('traffic_hours_ts.csv', index=False)\n",
    "    return None\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "def create_traffic_weekday_ts(files):\n",
    "    \n",
    "    df_agg = read_saved(files)\n",
    "\n",
    "    # create day column\n",
    "    df_agg['ts_h'] = pd.to_datetime(df_agg['ts_h'])\n",
    "    df_agg['weekday'] = df_agg['ts_h'].dt.weekday\n",
    "    # df_agg['weekday'] = df_agg['ts_h'].dt.weekday\n",
    "\n",
    "\n",
    "    df_agg = df_agg.groupby(['point_index', 'weekday'])['unique_device'].median().reset_index()\n",
    "\n",
    "\n",
    "    # add lat lon columns from grid\n",
    "    df_gr = pd.DataFrame(grid).reset_index()\n",
    "    df_gr.columns=['point_index', 'point_lat', 'point_lon']\n",
    "    df_fin = pd.merge(df_gr, df_agg, on=['point_index'], how='left').fillna(0)\n",
    "\n",
    "    # pivot - unpivot to fill missing weekday values\n",
    "    df_fin = pd.pivot_table(df_fin,\n",
    "                            values='unique_device',\n",
    "                            index='point_index',\n",
    "                            columns='weekday',\n",
    "                            fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "    df_fin = pd.melt(df_fin,\n",
    "                     id_vars='point_index',\n",
    "                     value_name='unique_device')\n",
    "\n",
    "    # check: df_fin[df_fin['point_index']==2914].sort_values(by='weekday')\n",
    "\n",
    "    # add lat lon\n",
    "    df_fin = pd.merge(df_fin, df_gr, on=['point_index'], how='left')\n",
    "\n",
    "    dfs = []\n",
    "    for weekday in df_fin['weekday'].unique():\n",
    "        print(weekday)\n",
    "        # get only values for specific weekday\n",
    "        dff = df_fin[df_fin['weekday']==weekday]\n",
    "        # blur points = make every point a sum from 6 surrounding points\n",
    "        blur = 18\n",
    "        if blur > 0:\n",
    "            indexes = tree.query(np.deg2rad(dff[['point_lat', 'point_lon']].to_numpy()),\n",
    "                                 k=blur)[1]\n",
    "            dff.index = dff['point_index'].astype(int)\n",
    "            rank_matrix = np.concatenate([dff[['unique_device']].reindex(indexes[:,idx]).to_numpy()\n",
    "                                          for idx in range(0,blur)],\n",
    "                                         axis=1)\n",
    "            dff['unique_device'] = np.sum(rank_matrix, axis=1)\n",
    "\n",
    "        # save\n",
    "        dff = dff.reset_index(drop=True)\n",
    "        dff = dff[['weekday', 'point_index', 'point_lat', 'point_lon', 'unique_device']]\n",
    "        dff.rename(columns={'unique_device': 'metric_traffic'})\n",
    "        dfs.append(dff)\n",
    "\n",
    "    df_final = pd.concat(dfs)\n",
    "    df_final.to_csv('traffic_weekday_ts.csv', index=False)\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_point_comps():\n",
    "    competition_data = pd.read_csv('competition_data.csv')\n",
    "    dfcom = competition_data[['ya_id', 'sub_cat']].groupby(['ya_id']).agg({\"sub_cat\": list}).reset_index()\n",
    "    dfcom['sub_cat'] = dfcom['sub_cat'].apply(lambda x: list(set(x)))\n",
    "\n",
    "\n",
    "    df_gr = pd.DataFrame(grid).reset_index()\n",
    "    df_gr.columns=['point_index', 'point_lat', 'point_lon']  \n",
    "    df_agg = competition_data.groupby('point_index').agg({'ya_id': list}).reset_index()\n",
    "\n",
    "    df_fin = pd.merge(df_gr, df_agg, on=['point_index'], how='left').fillna(0)\n",
    "    \n",
    "    def get_unique(m):\n",
    "        main_lst = []\n",
    "        for row in m:\n",
    "            lst = []\n",
    "            for elem in row:\n",
    "                if isinstance(elem, list):\n",
    "                    lst += elem\n",
    "            main_lst.append(list(set(lst)))\n",
    "\n",
    "        return main_lst\n",
    "\n",
    "\n",
    "    def map_id_to_categories(list_id):\n",
    "        dc = dfcom[dfcom['ya_id'].isin(list_id)].values.tolist()\n",
    "        newdict = {}\n",
    "        for key, val in dc:\n",
    "            for string in val:\n",
    "                newdict.setdefault(string, []).append(key)\n",
    "\n",
    "        return newdict\n",
    "\n",
    "    def map_full(rank_matrix):\n",
    "        return [map_id_to_categories(row) for row in get_unique(rank_matrix)]\n",
    "\n",
    "\n",
    "    indexes = tree.query(np.deg2rad(df_fin[['point_lat', 'point_lon']].to_numpy()),\n",
    "                     k=blur)[1]\n",
    "    df_fin.index = df_fin['point_index'].astype(int)\n",
    "    rank_matrix = np.concatenate([df_fin[['ya_id']].reindex(indexes[:,idx]).to_numpy()\n",
    "                                  for idx in range(0,blur)],\n",
    "                                 axis=1)\n",
    "    print('map_full')\n",
    "    df_fin['ya_id'] = map_full(rank_matrix)\n",
    "    df_fin = df_fin[['point_index', 'ya_id']].rename(columns={\"ya_id\": \"comps_array\"}).reset_index(drop=True)\n",
    "    df_fin.to_csv('point_comps.csv', index=False)\n",
    "    return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_point_info():\n",
    "    dff = pd.read_csv('traffic_weekday_ts.csv')\n",
    "    dff.sort_values(by=['point_index', 'weekday'], inplace=True)\n",
    "    dff['unique_device'] = dff['unique_device'].astype(int)\n",
    "\n",
    "    dff_wd = dff.groupby(['point_index']).agg(\n",
    "        {'unique_device': list}\n",
    "    ).reset_index().rename(columns={'unique_device': 'unique_device_weekday'})\n",
    "\n",
    "\n",
    "    dff = pd.read_csv('traffic_hours_ts.csv')\n",
    "    dff.sort_values(by=['point_index', 'hour'], inplace=True)\n",
    "    dff['unique_device'] = dff['unique_device'].astype(int)\n",
    "\n",
    "\n",
    "    dff_hr = dff.groupby(['point_index']).agg(\n",
    "        {'unique_device': list}\n",
    "    ).reset_index().rename(columns={'unique_device': 'unique_device_hour'})\n",
    "    \n",
    "    \n",
    "    df_comps = pd.read_csv('point_comps.csv')\n",
    "    \n",
    "    dff_ts = pd.merge(dff_wd, dff_hr, on='point_index', how='left')\n",
    "    dff_ts = pd.merge(dff_ts, df_comps , on='point_index', how='left')\n",
    "    \n",
    "    \n",
    "    dff_ts.to_csv('point_info.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(geojs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_info = pd.read_csv('point_info.csv')\n",
    "geojs = geojs\n",
    "clicked_point = 2674\n",
    "\n",
    "sub_cat = 'Магазин продуктов'\n",
    "\n",
    "\n",
    "def make_point_panel(geojs, clicked_point, sub_cat):\n",
    "    \n",
    "    mp = geojson_to_df(geojs)\n",
    "    mp = mp[mp['point_index']==clicked_point]\n",
    "    pi = point_info[point_info['point_index']==clicked_point]\n",
    "    \n",
    "    # get competitors dataframe\n",
    "    comps = json.loads(pi['comps_array'].values[0].replace(\"'\", '\"'))\n",
    "    comps = comps[sub_cat]\n",
    "    comps = competition_data[competition_data['ya_id'].isin(comps)].drop_duplicates(subset=['lat', 'long'])\n",
    "    point_coords = mp['point_lat'].values[0], mp['point_lon'].values[0]\n",
    "    comps['dist'] = comps.apply(lambda row: haversine_exc((row.lat, row.long), point_coords), axis=1)\n",
    "    comps['dist'] = comps['dist'].round(5)\n",
    "    comps['lat'] = comps['lat'].round(5)\n",
    "    comps['long'] = comps['long'].round(5)\n",
    "    comps['address'] = comps['address'].str.replace('Россия, Москва,','')\n",
    "    comps = comps[['name','lat', 'long', 'dist', 'address', 'hours_text']].sort_values(by='dist').reset_index(drop=True)\n",
    "    comps.columns = ['Название', 'Широта', 'Долгота', 'Расстояние, км', 'Адрес', 'Часы работы']\n",
    "    \n",
    "    \n",
    "    # get traffic_hours timeseries\n",
    "    ts_hour = json.loads(pi['unique_device_hour'].values[0])\n",
    "    total = sum(ts_hour)\n",
    "    ts_hour = pd.DataFrame({str(idx): (hour+1)/(total+1) for idx, hour in enumerate(ts_hour)}, index=[0]).T.reset_index()\n",
    "    ts_hour.columns = ['Час', 'Уровень траффика (%)']\n",
    "    \n",
    "    # create weekly ts\n",
    "    ts_weekday = json.loads(pi['unique_device_weekday'].values[0])\n",
    "    total = sum(ts_weekday)\n",
    "    mapping_wd = {\n",
    "        0: 'Пн',\n",
    "        1: 'Вт',\n",
    "        2: 'Ср',\n",
    "        3: 'Чт',\n",
    "        4: 'Пт',\n",
    "        5:'Сб',\n",
    "        6:'Вс'\n",
    "    }\n",
    "    ts_weekday = pd.DataFrame(\n",
    "        {mapping_wd[idx]: (weekday+1)/(total+1) for idx, weekday in enumerate(ts_weekday)}, index=[0]).T.reset_index()\n",
    "    ts_weekday.columns = ['День', 'Уровень траффика (%)']\n",
    "    \n",
    "    \n",
    "    # create common info\n",
    "    mp_1 = mp[['point_index', 'point_lat', 'point_lon', 'metric_rent', 'metric_traffic', 'count_comps', 'total_rank']]\n",
    "\n",
    "    mp_1['metric_rent'] = (mp_1['metric_rent']/38).astype(int).astype(str)\n",
    "    mp_1['metric_traffic'] = mp_1['metric_traffic'].astype(int).astype(str)\n",
    "    mp_1['point_lat'] = mp_1['point_lat'].round(5).astype(str)\n",
    "    mp_1['point_lon'] = mp_1['point_lon'].round(5).astype(str)\n",
    "\n",
    "    mp_1.columns = ['Локация', 'Широта', 'Долгота',\n",
    "                    'Аренда (руб. в мес. за кв.м.)', 'Траффик (чел. в мес., радиус 0.5 км)',\n",
    "                    'Конкуренты (шт., радиус 0.5 км)', 'Рейтинг']\n",
    "#     mp_1 = mp_1.T.reset_index()\n",
    "#     mp_1.columns = ['Показатель', 'Значение']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # build figure\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=3,\n",
    "        column_widths=[0.22, 0.18, 0.6],\n",
    "        specs=[[{'type': 'table', \"rowspan\": 2}, {}, {'type': 'table',\"rowspan\": 2}],\n",
    "               [{},           {}, {}]],\n",
    "        subplot_titles=('О локации ', 'Уровень траффика по часам', 'Ближайшие конкуренты', '', 'Уровень траффика по дням')\n",
    "\n",
    "    )\n",
    "\n",
    "    trace_general = go.Table(\n",
    "        #columnwidth = [150,100],\n",
    "        header=dict(values=list(mp_1.columns),\n",
    "                    fill_color='#EEF6FF',\n",
    "                    align='left'),\n",
    "        cells=dict(values=[mp_1[col] for col in mp_1.columns],\n",
    "                   fill_color='white',\n",
    "                   align='left',\n",
    "                  height=60))\n",
    "\n",
    "\n",
    "    trace_wd = px.bar(x='День',\n",
    "           y='Уровень траффика (%)',\n",
    "           data_frame=ts_weekday)\n",
    "\n",
    "    trace_wd.update_traces(marker_color='#EEF6FF')\n",
    "    trace_wd.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')\n",
    "    trace_wd.update_yaxes(color='white')\n",
    "\n",
    "    trace_hr = px.bar(x='Час',\n",
    "           y='Уровень траффика (%)',\n",
    "           data_frame=ts_hour)\n",
    "\n",
    "    trace_hr.update_traces(marker_color='#EEF6FF')\n",
    "    trace_hr.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')\n",
    "    trace_hr.update_yaxes(color='white')\n",
    "\n",
    "\n",
    "\n",
    "    trace_comps = go.Table(\n",
    "        header=dict(values=list(comps.columns),\n",
    "                    fill_color='#EEF6FF',\n",
    "                    align='left'),\n",
    "        cells=dict(values=[comps[col] for col in comps.columns],\n",
    "                   fill_color='white',\n",
    "                   align='left'))\n",
    "\n",
    "\n",
    "\n",
    "    fig.add_trace(trace_general, row=1, col=1)\n",
    "    fig.add_trace(trace_hr['data'][0], row=1, col=2)\n",
    "    fig.add_trace(trace_wd['data'][0], row=2, col=2)\n",
    "    fig.add_trace(trace_comps, row=1, col=3)\n",
    "    fig.update_layout(width=1300, height=600)\n",
    "\n",
    "    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')\n",
    "    fig.update_yaxes(color='white')\n",
    "\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
